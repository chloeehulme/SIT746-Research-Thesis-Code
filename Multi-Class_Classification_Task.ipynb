{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model C (Fine-tuned multi-class classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories for heat maps\n",
    "train_dir = './multiclass_test/training_set'\n",
    "test_dir = './multiclass_test/test_set'\n",
    "\n",
    "# Define parameters\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "\n",
    "# Define ImageDataGenerator\n",
    "training_gen = ImageDataGenerator(validation_split=validation_split, rescale=1./255)\n",
    "test_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(64, (5, 5), activation='relu', input_shape=(img_height, img_width, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (5, 5), activation='tanh'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(96, (5, 5), activation='tanh'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "          \n",
    "model.add(layers.Dropout(0.3))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(96, activation='relu'))\n",
    "\n",
    "model.add(layers.Dropout(0.3))\n",
    "\n",
    "# Output Layer\n",
    "model.add(layers.Dense(9, activation='softmax'))\n",
    "\n",
    "# Choose optimizer and learning rate\n",
    "learning_rate = 0.0011228308916583806\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "csv_filename = './multiclass_test/tuning/final_trial_results_2.csv'\n",
    "\n",
    "\n",
    "# Define the column names\n",
    "fieldnames = ['trial_num', 'test_acc', 'test_loss', 'precision', 'recall', 'Analysis', 'Backdoors', 'DoS', \n",
    "              'Exploits', 'Fuzzers', 'Generic', 'Non-attack', 'Reconnaissance', 'Shellcode']\n",
    "\n",
    "column_map = {\n",
    "                0: 'Analysis',\n",
    "                1: 'Backdoors',\n",
    "                2: 'DoS',\n",
    "                3: 'Exploits',\n",
    "                4: 'Fuzzers',\n",
    "                5: 'Generic',\n",
    "                6: 'Non-attack',\n",
    "                7: 'Reconnaissance',\n",
    "                8: 'Shellcode'\n",
    "            }\n",
    "\n",
    "\n",
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for n in range(30):\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Pull images from directories and assign labels\n",
    "        train_generator = training_gen.flow_from_directory(\n",
    "            train_dir,\n",
    "            target_size=(img_height, img_width),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='training')\n",
    "        \n",
    "        validation_generator = training_gen.flow_from_directory(\n",
    "            train_dir,\n",
    "            target_size=(img_height, img_width),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='validation')\n",
    "        \n",
    "        test_generator = test_gen.flow_from_directory(\n",
    "            test_dir,\n",
    "            target_size=(img_height, img_width),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            shuffle=False)\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            epochs=50,  # maximum number of epochs\n",
    "            validation_data=validation_generator,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "    \n",
    "        # Evaluation\n",
    "        test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "        \n",
    "        # Make Predictions\n",
    "        predictions = model.predict(test_generator)\n",
    "        integer_predictions = np.argmax(predictions, axis=1)\n",
    "        true_labels = test_generator.classes\n",
    "        \n",
    "        # Overall precision and recall\n",
    "        precision = precision_score(true_labels, integer_predictions, average='macro')\n",
    "        recall = recall_score(true_labels, integer_predictions, average='macro')\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        conf_matrix = confusion_matrix(true_labels, integer_predictions)\n",
    "\n",
    "        # Prepare the row data\n",
    "        row_data = {\n",
    "            'trial_num': n,\n",
    "            'test_acc': test_accuracy,\n",
    "            'test_loss': test_loss,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "        # Metrics for each class\n",
    "        for i in range(conf_matrix.shape[0]):\n",
    "            TP = conf_matrix[i, i]\n",
    "            FP = conf_matrix[:, i].sum() - TP\n",
    "            FN = conf_matrix[i, :].sum() - TP\n",
    "            TN = conf_matrix.sum() - (FP + FN + TP)\n",
    "            FPR = FP / (FP + TN) if (FP + TN) != 0 else 0\n",
    "        \n",
    "            # Calculate precision and recall\n",
    "            class_precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "            class_recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "\n",
    "            # Add class-specific metrics to the row data\n",
    "            row_data[column_map[i]] = f'TP: {TP} - FP: {FP} - FN: {FN} - TN: {TN} - FPR: {FPR:.4f} - Pr: {class_precision:.4f} - Re: {class_recall:.4f}'\n",
    "\n",
    "\n",
    "        # Write the row to the CSV file\n",
    "        writer.writerow(row_data)\n",
    "        \n",
    "        print(f\"Trial {n} saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "from tensorflow.keras import models, layers, callbacks\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "target_size = (224, 224)\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "validation_split = 0.2\n",
    "\n",
    "# Define the parameter ranges for tuning\n",
    "num_conv_layers = [2, 3, 4]\n",
    "batch_sizes = [32, 64, 128]\n",
    "kernel_sizes = [(3, 3), (5, 5), (7, 7)]\n",
    "\n",
    "# Define directories for heat maps\n",
    "train_dir = './multiclass_test/training_set'\n",
    "test_dir = './multiclass_test/test_set'\n",
    "\n",
    "# Define ImageDataGenerator\n",
    "training_gen = ImageDataGenerator(validation_split=validation_split)\n",
    "test_gen = ImageDataGenerator()\n",
    "training_gen.rescale = 1. / 255.0\n",
    "test_gen.rescale = 1. / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare CSV file\n",
    "csv_filename = './multiclass_test/multi_class_tuning_results.csv'\n",
    "csv_header = ['num_conv_layers', 'batch_size', 'kernel_size', 'test_accuracy', 'precision', 'recall', 'avg_fpr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and tune\n",
    "with open(csv_filename, 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(csv_header)\n",
    "\n",
    "    # Iterate through all combinations of parameters\n",
    "    for num_layers, batch_size, kernel_size in itertools.product(num_conv_layers, batch_sizes, kernel_sizes):\n",
    "        print(f\"Training with {num_layers} conv layers, batch size {batch_size}, kernel size {kernel_size}\")\n",
    "\n",
    "        # Pull images from directories and assign labels\n",
    "        train_generator = training_gen.flow_from_directory(\n",
    "            train_dir,\n",
    "            target_size=target_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='training')\n",
    "\n",
    "        validation_generator = training_gen.flow_from_directory(\n",
    "            train_dir,\n",
    "            target_size=target_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='validation')\n",
    "\n",
    "        test_generator = test_gen.flow_from_directory(\n",
    "            test_dir,\n",
    "            target_size=target_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            shuffle=False)\n",
    "\n",
    "        # Build the model\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(32, kernel_size, activation='relu', input_shape=(img_height, img_width, 3)))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            model.add(layers.Conv2D(64, kernel_size, activation='relu'))\n",
    "            model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(64, activation='relu'))\n",
    "        model.add(layers.Dense(9, activation='softmax'))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping = callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # Training\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            epochs=50,\n",
    "            validation_data=validation_generator,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "\n",
    "        # Get the number of epochs actually trained\n",
    "        epochs_trained = len(history.history['loss'])\n",
    "\n",
    "        # Evaluation\n",
    "        test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "\n",
    "        # Make Predictions\n",
    "        predictions = model.predict(test_generator)\n",
    "        integer_predictions = np.argmax(predictions, axis=1)\n",
    "        true_labels = test_generator.classes\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        precision = precision_score(true_labels, integer_predictions, average='macro')\n",
    "        recall = recall_score(true_labels, integer_predictions, average='macro')\n",
    "\n",
    "        # Calculate confusion matrix\n",
    "        conf_matrix = confusion_matrix(true_labels, integer_predictions)\n",
    "\n",
    "        # Calculate FPR for each class and then average\n",
    "        fprs = []\n",
    "        for i in range(conf_matrix.shape[0]):\n",
    "            FP = conf_matrix[:, i].sum() - conf_matrix[i, i]\n",
    "            TN = conf_matrix.sum() - (conf_matrix[i, :].sum() + conf_matrix[:, i].sum() - conf_matrix[i, i])\n",
    "            FPR = FP / (FP + TN) if (FP + TN) != 0 else 0\n",
    "            fprs.append(FPR)\n",
    "\n",
    "        # Calculate average FPR\n",
    "        avg_fpr = np.mean(fprs)\n",
    "\n",
    "        # Save results to CSV\n",
    "        with open(csv_filename, 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([num_layers, batch_size, kernel_size, test_accuracy, precision, recall, avg_fpr, epochs_trained])\n",
    "\n",
    "print(f\"Results saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import keras_tuner as kt\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from random import sample\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories for heat maps\n",
    "train_dir = './multiclass_test/training_set'\n",
    "test_dir = './multiclass_test/test_set'\n",
    "\n",
    "# Define parameters\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "\n",
    "# Define ImageDataGenerator\n",
    "training_gen = ImageDataGenerator(validation_split=validation_split, rescale=1./255)\n",
    "test_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull images from directories and assign labels\n",
    "train_generator = training_gen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training')\n",
    "\n",
    "validation_generator = training_gen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')\n",
    "\n",
    "test_generator = test_gen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Convolutional layers\n",
    "    for i in range(3):\n",
    "        model.add(layers.Conv2D(\n",
    "            hp.Int(f'conv_{i+1}_filters', 32, 128, step=32),\n",
    "            (5, 5),\n",
    "            activation=hp.Choice(f'conv_{i+1}_activation', ['relu', 'tanh', 'sigmoid']),\n",
    "            input_shape=(img_height, img_width, 3) if i == 0 else None\n",
    "        ))\n",
    "        model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "        # Optional Dropout after conv layer\n",
    "        if hp.Boolean(f'dropout_after_conv_{i+1}'):\n",
    "            model.add(layers.Dropout(hp.Float(f'dropout_rate_conv_{i+1}', 0.1, 0.5, step=0.1)))\n",
    "\n",
    "\n",
    "    # Flatten Layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Dense Layer\n",
    "    model.add(layers.Dense(\n",
    "        hp.Int('dense_units', 32, 128, step=32),\n",
    "        activation=hp.Choice('dense_activation', ['relu', 'tanh', 'sigmoid'])\n",
    "    ))\n",
    "\n",
    "    # Optional Dropout after dense layer\n",
    "    if hp.Boolean('dropout_after_dense'):\n",
    "        model.add(layers.Dropout(hp.Float('dropout_rate_dense', 0.1, 0.5, step=0.1)))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(9, activation='softmax'))\n",
    "\n",
    "    # Choose optimizer and learning rate\n",
    "    optimizer_name = hp.Choice('optimizer', ['adam', 'rmsprop', 'sgd'])\n",
    "    learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n",
    "\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    else:  # sgd\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = './multiclass_test/tuning'\n",
    "\n",
    "# Define the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=50,  # number of different hyperparameter combinations to try\n",
    "    executions_per_trial=1,\n",
    "    directory=local_dir,\n",
    "    project_name='image_classification_70'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping callback with tunable patience\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the search\n",
    "tuner.search(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Save best hyperparameters to CSV\n",
    "csv_filename = './multiclass_test/tuning/best_hyper_params.csv'\n",
    "\n",
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write header and data\n",
    "    for param, value in best_hps.values.items():\n",
    "        writer.writerow([param, value])\n",
    "\n",
    "print(f\"Best hyperparameters saved to {csv_filename}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
